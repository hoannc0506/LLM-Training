# Finetune and align Llama-3 for summarization

## Efficient Fintune
- QLoRA
- QLoRA with Unsloth
- DDP and FSDP

## Align
- RLHF
- DPO

## Result
- Train SFT (not eval): ~ 30GB GPU
- Train with QLoRA: ~15GB GPU
- Train FSDP

## TODO
- [ ] Evaluate SFT model and LoRA model
- [ ] Public HF models
- [ ] Train FSDP